# COT-RAG-Agent

Finetuning a model using UNSLOTH to use as a retriever for a RAG.

You will find the following notebooks:  
- **Inference** `inference.ipynb`: Connect to your vector DB (I'm using Chroma DB).  
- **Synthetic Data Generation** `generate_dataset.ipynb`: Generates the synthetic dataset that we will use to finetune our model **(CHECK THE TERMS AND CONDITIONS OF THE MODEL YOU USE TO GENERATE THE DATASET)**  
- **Finetuning** `finetune.ipynb`: Finetunes the model using UNSLOTH.

## How does it work?

We finetune a model on our dataset that contains the following tags:  
- `<think>`: Block where the chain-of-thought happens.  
- `<query>`: Marks where the model calls the vector DB’s search function. Takes only the query as parameter.  
- `<query_res>`: Tag where the search results of the previous search are injected.  
- `<results>`: Final tag generated by the LLM after the CoT concludes, containing the vetted search results.

Our model generates blocks of 1000 tokens. After each generation cycle, we check if the splrinkled tags mentioned upward are present in the text. If the `<query>` tag is found, we extract the query, search the vector DB, and inject the `<query_res>` into the generated text. This updated input is then fed back to the model. The process repeats until the `<results>` tag is encountered.

TBH, it doesn't work as well as I hoped. Probably because of my limited and low-quality dataset. It was mostly just a fun experiment that I did on a cold weeknd.

If you want to check out the model check my HF [Martingkc/llama_lora_merged_model_v3](https://huggingface.co/Martingkc/llama_lora_merged_model_v3) — all the models that you see as `llama_lora` are the LoRA adapters. Don’t worry about merging them UNSLOTH does it automatically during inference (check their documentation). The models that contain `merged` in their names are the merged ones, and there are also the `mlx` versions of the model, which work extremely slowly on my M1 MBP 16GB.

## TODOs:
1. Upload code ahahah  
2. Code cleanup  
